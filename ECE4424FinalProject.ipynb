{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE4424FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mabalzich/MLFinalProject/blob/main/ECE4424FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Nt3oOBuLE2_x"
      },
      "outputs": [],
      "source": [
        "# imported libraries\n",
        "# a lot are similar to homework 4\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import backend as K\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for this script\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "learningrate = 0.0001"
      ],
      "metadata": {
        "id": "NjwPEbJcE97V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recall function from keras documentation\n",
        "def recallscore(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall"
      ],
      "metadata": {
        "id": "5uLj3IMtlpfQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#precision function from keras documentation\n",
        "def precisionscore(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "metadata": {
        "id": "8pCHyJaIlsjn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1 score from keras documentation\n",
        "def f1score(y_true, y_pred):\n",
        "    precision = precisionscore(y_true, y_pred)\n",
        "    recall = recallscore(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "S7JF9Df6lvyQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data():\n",
        "  # Load and read splited training and test data set\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()  \n",
        "  print('x_train shape:', x_train.shape)\n",
        "  print(x_train.shape[0], 'train samples')\n",
        "  print(x_test.shape[0], 'test samples')\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "  # normalize the data\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  # partition training set into training and validation set\n",
        "  x_validate = x_train[50000:,:]\n",
        "  x_train = x_train[:50000,:]\n",
        "  y_validate = y_train[50000:,:]\n",
        "  y_train = y_train[:50000,:]\n",
        "\n",
        "  return x_train, y_train, x_validate, y_validate, x_test, y_test"
      ],
      "metadata": {
        "id": "MuudUbPcWILs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_validate, y_validate, x_test, y_test = generate_data()"
      ],
      "metadata": {
        "id": "R_dTIugmMLdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb57864-27e4-493c-e43e-b65410629d10"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def base_cnn():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv1D(28, 3, padding='same',input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv1D(28, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        " \n",
        "  model.add(Conv1D(56, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv1D(56, 3, padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  opt = keras.optimizers.RMSprop(learning_rate=learningrate, decay=1e-6)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy', f1score, precisionscore, recallscore])\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "1sxKGnJFFZtx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphNet(nn.Module):\n",
        "    def __init__(self, image_size = 28):\n",
        "        super(GraphNet, self).__init__()\n",
        "        N = image_size ** 2 # Number of pixels in the image\n",
        "        self.fc = nn.Linear(N, 10, bias = False)\n",
        "        # Create the adjacency matrix of size (N X N)\n",
        "        # Use a pre-computed adjacency matrix\n",
        "        A = self.precompute_adjacency_images(image_size)\n",
        "        self.register_buffer('A', A) # not to be considered a model paramater that is updated during training\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: image (batch_size x 1 x image_width x image_height)\n",
        "        '''\n",
        "        B = x.size(0) # 64\n",
        "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1), \n",
        "                                            x.view(B, -1, 1)).view(B, -1)) # (64 X 784)\n",
        "        return self.fc(avg_neighbor_features)\n",
        "\n",
        "    @staticmethod\n",
        "    # Static method knows nothing about the class and just deals with the parameters.\n",
        "    def precompute_adjacency_images(image_size):\n",
        "        print('precompute_adjacency_images')\n",
        "        col, row = np.meshgrid(np.arange(image_size), np.arange(image_size)) # (28 x 28) Explanation: https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
        "        coord = np.stack((col, row), axis = 2).reshape(-1, 2) / image_size # (784 x 2) --> normalize\n",
        "        dist = cdist(coord, coord) # compute distance between every pair of pixels\n",
        "        sigma = 0.05 * np.pi # width of the Gaussian (can be a hyperparameter while training a model)\n",
        "        A = np.exp(-dist / sigma ** 2) # adjacency matrix of spatial similarity\n",
        "        A[A < 0.01] = 0 # suppress values less than 0.01\n",
        "        A = torch.from_numpy(A).float()\n",
        "\n",
        "        # Normalization as per (Kipf & Welling, ICLR 2017)\n",
        "        D = A.sum(1)  # nodes degree (N,)\n",
        "        D_hat = (D + 1e-5) ** (-0.5)\n",
        "        A_hat = D_hat.view(-1, 1) * A * D_hat.view(1, -1)  # N,N\n",
        "\n",
        "        # Some additional trick I found to be useful\n",
        "        A_hat[A_hat > 0.0001] = A_hat[A_hat > 0.0001] - 0.2\n",
        "\n",
        "        print(A_hat[:10, :10])\n",
        "        return A_hat"
      ],
      "metadata": {
        "id": "GznC9hzbLahQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    accuracy = []\n",
        "    f1score = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    p_inter = [[0,0] for x in range(10)]\n",
        "    r_inter = [[0,0] for x in range(10)]\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    next = 1\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        for p, t, c in zip(pred,target,pred.eq(target.view_as(pred))):\n",
        "          p_inter[p.item()][0] += c.item()\n",
        "          p_inter[p.item()][1] += 1\n",
        "          r_inter[p.item()][0] += c.item()\n",
        "          r_inter[t.item()][1] += 1\n",
        "        total += len(data)\n",
        "        if total > (next * 10000):\n",
        "          next += 1\n",
        "          #macro averaging\n",
        "          prec = sum([1.0 * x[0]/x[1] for x in p_inter]) / 10.0\n",
        "          rec = sum([1.0 * x[0]/x[1] for x in r_inter]) / 10.0\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: ({:.2f}%) F1 Score: ({:.2f}%) Precision: ({:.2f}%) Recall: ({:.2f}%)'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item(),\n",
        "              100. * correct / total, 100. * 2*((prec*rec)/(prec+rec+1e-12)), 100. * prec, 100. * rec))\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: ({:.2f}%) F1 Score: ({:.2f}%) Precision: ({:.2f}%) Recall: ({:.2f}%)'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item(),\n",
        "              100. * correct / total, 100. * 2*((prec*rec)/(prec+rec+1e-12)), 100. * prec, 100. * rec))\n",
        "    prec = sum([1.0 * x[0]/x[1] for x in p_inter]) / 10.0\n",
        "    rec = sum([1.0 * x[0]/x[1] for x in r_inter]) / 10.0\n",
        "    return 1.0 * correct / total, 2*((prec*rec)/(prec+rec+1e-12)), prec, rec"
      ],
      "metadata": {
        "id": "EE1ntETmLcJY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return 1. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "CvXzhk3wLglv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  base_cnn()\n",
        "fit = model.fit(x_train, y_train,\n",
        "                                batch_size=batch_size,\n",
        "                                epochs=epochs,\n",
        "                                validation_data=(x_validate, y_validate),\n",
        "                                shuffle=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=batch_size, shuffle=False)    \n",
        "\n",
        "model = GraphNet()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = learningrate, weight_decay = 1e-4)\n",
        "\n",
        "print('number of trainable parameters: %d' %\n",
        "      np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n",
        "\n",
        "accuracy = []\n",
        "f1scores = []\n",
        "precision = []\n",
        "recall = []\n",
        "val_accuracy = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "  acc, f1, prec, rec = train(model, train_loader, optimizer, epoch)\n",
        "  accuracy.append(acc)\n",
        "  f1scores.append(f1)\n",
        "  precision.append(prec)\n",
        "  recall.append(rec)\n",
        "  val_accuracy.append(test(model, test_loader))\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.plot(fit.history['accuracy'], 'o-', label='CNN')\n",
        "plt.plot([item for item in accuracy if item is not None], 'o-', label='GNN')\n",
        "\n",
        "plt.title('training accuracy')\n",
        "plt.ylabel('training accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.plot(fit.history['val_accuracy'], 'o-', label='CNN')\n",
        "plt.plot([item for item in val_accuracy if item is not None], 'o-', label='GNN')\n",
        "\n",
        "plt.title('validation accuracy')\n",
        "plt.ylabel('validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Plot f1 score\n",
        "plt.plot(fit.history['f1score'], 'o-', label='CNN')\n",
        "plt.plot([item for item in f1scores if item is not None], 'o-', label='GNN')\n",
        "\n",
        "plt.title('f1 score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Plot precision\n",
        "plt.plot(fit.history['precisionscore'], 'o-', label='CNN')\n",
        "plt.plot([item for item in precision if item is not None], 'o-', label='GNN')\n",
        "\n",
        "plt.title('precision')\n",
        "plt.ylabel('precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Plot recall\n",
        "plt.plot(fit.history['recallscore'], 'o-', label='CNN')\n",
        "plt.plot([item for item in recall if item is not None], 'o-', label='GNN')\n",
        "\n",
        "plt.title('recall')\n",
        "plt.ylabel('recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EWrfKCbZLRdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d4f75b-88fe-48a3-9555-1f77e9cde302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 28, 28)            2380      \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 28, 28)            0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 28, 28)            2380      \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 28, 28)            0         \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 14, 28)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 14, 28)            0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 14, 56)            4760      \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 14, 56)            0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 14, 56)            9464      \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 14, 56)            0         \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 7, 56)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 7, 56)             0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 392)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               201216    \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,330\n",
            "Trainable params: 225,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 17s 10ms/step - loss: 1.0393 - accuracy: 0.6597 - f1score: 0.5721 - precisionscore: 0.7147 - recallscore: 0.5045 - val_loss: 0.2866 - val_accuracy: 0.9150 - val_f1score: 0.9145 - val_precisionscore: 0.9426 - val_recallscore: 0.8890\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.3847 - accuracy: 0.8811 - f1score: 0.8803 - precisionscore: 0.9101 - recallscore: 0.8534 - val_loss: 0.1766 - val_accuracy: 0.9457 - val_f1score: 0.9470 - val_precisionscore: 0.9592 - val_recallscore: 0.9356\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.2725 - accuracy: 0.9169 - f1score: 0.9174 - precisionscore: 0.9343 - recallscore: 0.9016 - val_loss: 0.1332 - val_accuracy: 0.9602 - val_f1score: 0.9608 - val_precisionscore: 0.9693 - val_recallscore: 0.9527\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2194 - accuracy: 0.9337 - f1score: 0.9345 - precisionscore: 0.9463 - recallscore: 0.9234 - val_loss: 0.1111 - val_accuracy: 0.9666 - val_f1score: 0.9676 - val_precisionscore: 0.9736 - val_recallscore: 0.9620\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1864 - accuracy: 0.9430 - f1score: 0.9432 - precisionscore: 0.9530 - recallscore: 0.9338 - val_loss: 0.0982 - val_accuracy: 0.9697 - val_f1score: 0.9703 - val_precisionscore: 0.9751 - val_recallscore: 0.9657\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1647 - accuracy: 0.9494 - f1score: 0.9497 - precisionscore: 0.9574 - recallscore: 0.9423 - val_loss: 0.0884 - val_accuracy: 0.9733 - val_f1score: 0.9735 - val_precisionscore: 0.9775 - val_recallscore: 0.9695\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1473 - accuracy: 0.9539 - f1score: 0.9544 - precisionscore: 0.9612 - recallscore: 0.9479 - val_loss: 0.0813 - val_accuracy: 0.9768 - val_f1score: 0.9771 - val_precisionscore: 0.9810 - val_recallscore: 0.9733\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1378 - accuracy: 0.9581 - f1score: 0.9581 - precisionscore: 0.9641 - recallscore: 0.9523 - val_loss: 0.0752 - val_accuracy: 0.9777 - val_f1score: 0.9784 - val_precisionscore: 0.9819 - val_recallscore: 0.9749\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1275 - accuracy: 0.9612 - f1score: 0.9618 - precisionscore: 0.9673 - recallscore: 0.9566 - val_loss: 0.0729 - val_accuracy: 0.9779 - val_f1score: 0.9785 - val_precisionscore: 0.9814 - val_recallscore: 0.9756\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1235 - accuracy: 0.9628 - f1score: 0.9631 - precisionscore: 0.9680 - recallscore: 0.9585 - val_loss: 0.0685 - val_accuracy: 0.9794 - val_f1score: 0.9804 - val_precisionscore: 0.9839 - val_recallscore: 0.9771\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1143 - accuracy: 0.9653 - f1score: 0.9658 - precisionscore: 0.9704 - recallscore: 0.9615 - val_loss: 0.0657 - val_accuracy: 0.9799 - val_f1score: 0.9810 - val_precisionscore: 0.9835 - val_recallscore: 0.9786\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1073 - accuracy: 0.9669 - f1score: 0.9671 - precisionscore: 0.9717 - recallscore: 0.9627 - val_loss: 0.0636 - val_accuracy: 0.9823 - val_f1score: 0.9829 - val_precisionscore: 0.9856 - val_recallscore: 0.9803\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1053 - accuracy: 0.9677 - f1score: 0.9680 - precisionscore: 0.9719 - recallscore: 0.9642 - val_loss: 0.0607 - val_accuracy: 0.9826 - val_f1score: 0.9826 - val_precisionscore: 0.9852 - val_recallscore: 0.9801\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0991 - accuracy: 0.9697 - f1score: 0.9702 - precisionscore: 0.9742 - recallscore: 0.9663 - val_loss: 0.0589 - val_accuracy: 0.9832 - val_f1score: 0.9836 - val_precisionscore: 0.9858 - val_recallscore: 0.9815\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0969 - accuracy: 0.9700 - f1score: 0.9705 - precisionscore: 0.9740 - recallscore: 0.9672 - val_loss: 0.0576 - val_accuracy: 0.9835 - val_f1score: 0.9843 - val_precisionscore: 0.9864 - val_recallscore: 0.9823\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0945 - accuracy: 0.9715 - f1score: 0.9719 - precisionscore: 0.9752 - recallscore: 0.9687 - val_loss: 0.0570 - val_accuracy: 0.9837 - val_f1score: 0.9842 - val_precisionscore: 0.9863 - val_recallscore: 0.9822\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0918 - accuracy: 0.9723 - f1score: 0.9723 - precisionscore: 0.9754 - recallscore: 0.9693 - val_loss: 0.0555 - val_accuracy: 0.9841 - val_f1score: 0.9848 - val_precisionscore: 0.9867 - val_recallscore: 0.9829\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0880 - accuracy: 0.9728 - f1score: 0.9730 - precisionscore: 0.9762 - recallscore: 0.9699 - val_loss: 0.0541 - val_accuracy: 0.9845 - val_f1score: 0.9850 - val_precisionscore: 0.9874 - val_recallscore: 0.9827\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0876 - accuracy: 0.9736 - f1score: 0.9742 - precisionscore: 0.9774 - recallscore: 0.9711 - val_loss: 0.0542 - val_accuracy: 0.9844 - val_f1score: 0.9851 - val_precisionscore: 0.9872 - val_recallscore: 0.9830\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0833 - accuracy: 0.9751 - f1score: 0.9754 - precisionscore: 0.9780 - recallscore: 0.9729 - val_loss: 0.0529 - val_accuracy: 0.9847 - val_f1score: 0.9851 - val_precisionscore: 0.9868 - val_recallscore: 0.9835\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0826 - accuracy: 0.9750 - f1score: 0.9751 - precisionscore: 0.9778 - recallscore: 0.9725 - val_loss: 0.0515 - val_accuracy: 0.9855 - val_f1score: 0.9861 - val_precisionscore: 0.9880 - val_recallscore: 0.9843\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0823 - accuracy: 0.9754 - f1score: 0.9760 - precisionscore: 0.9790 - recallscore: 0.9732 - val_loss: 0.0533 - val_accuracy: 0.9847 - val_f1score: 0.9852 - val_precisionscore: 0.9870 - val_recallscore: 0.9835\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0806 - accuracy: 0.9750 - f1score: 0.9754 - precisionscore: 0.9782 - recallscore: 0.9728 - val_loss: 0.0529 - val_accuracy: 0.9852 - val_f1score: 0.9859 - val_precisionscore: 0.9876 - val_recallscore: 0.9842\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0747 - accuracy: 0.9770 - f1score: 0.9772 - precisionscore: 0.9796 - recallscore: 0.9750 - val_loss: 0.0544 - val_accuracy: 0.9847 - val_f1score: 0.9851 - val_precisionscore: 0.9867 - val_recallscore: 0.9835\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0772 - accuracy: 0.9767 - f1score: 0.9767 - precisionscore: 0.9795 - recallscore: 0.9740 - val_loss: 0.0529 - val_accuracy: 0.9853 - val_f1score: 0.9856 - val_precisionscore: 0.9872 - val_recallscore: 0.9840\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0768 - accuracy: 0.9769 - f1score: 0.9769 - precisionscore: 0.9792 - recallscore: 0.9747 - val_loss: 0.0510 - val_accuracy: 0.9850 - val_f1score: 0.9856 - val_precisionscore: 0.9877 - val_recallscore: 0.9836\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0734 - accuracy: 0.9778 - f1score: 0.9780 - precisionscore: 0.9804 - recallscore: 0.9757 - val_loss: 0.0506 - val_accuracy: 0.9851 - val_f1score: 0.9853 - val_precisionscore: 0.9875 - val_recallscore: 0.9831\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0727 - accuracy: 0.9780 - f1score: 0.9781 - precisionscore: 0.9803 - recallscore: 0.9759 - val_loss: 0.0514 - val_accuracy: 0.9851 - val_f1score: 0.9853 - val_precisionscore: 0.9869 - val_recallscore: 0.9837\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0733 - accuracy: 0.9781 - f1score: 0.9781 - precisionscore: 0.9804 - recallscore: 0.9759 - val_loss: 0.0527 - val_accuracy: 0.9850 - val_f1score: 0.9851 - val_precisionscore: 0.9867 - val_recallscore: 0.9835\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0727 - accuracy: 0.9772 - f1score: 0.9777 - precisionscore: 0.9800 - recallscore: 0.9755 - val_loss: 0.0495 - val_accuracy: 0.9862 - val_f1score: 0.9866 - val_precisionscore: 0.9880 - val_recallscore: 0.9853\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0690 - accuracy: 0.9795 - f1score: 0.9796 - precisionscore: 0.9816 - recallscore: 0.9777 - val_loss: 0.0509 - val_accuracy: 0.9850 - val_f1score: 0.9853 - val_precisionscore: 0.9871 - val_recallscore: 0.9836\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0714 - accuracy: 0.9781 - f1score: 0.9782 - precisionscore: 0.9804 - recallscore: 0.9761 - val_loss: 0.0510 - val_accuracy: 0.9855 - val_f1score: 0.9857 - val_precisionscore: 0.9877 - val_recallscore: 0.9838\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0686 - accuracy: 0.9795 - f1score: 0.9798 - precisionscore: 0.9817 - recallscore: 0.9779 - val_loss: 0.0490 - val_accuracy: 0.9867 - val_f1score: 0.9870 - val_precisionscore: 0.9885 - val_recallscore: 0.9855\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0683 - accuracy: 0.9796 - f1score: 0.9798 - precisionscore: 0.9819 - recallscore: 0.9777 - val_loss: 0.0491 - val_accuracy: 0.9860 - val_f1score: 0.9862 - val_precisionscore: 0.9884 - val_recallscore: 0.9841\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0679 - accuracy: 0.9789 - f1score: 0.9795 - precisionscore: 0.9816 - recallscore: 0.9775 - val_loss: 0.0483 - val_accuracy: 0.9866 - val_f1score: 0.9867 - val_precisionscore: 0.9884 - val_recallscore: 0.9851\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0674 - accuracy: 0.9796 - f1score: 0.9800 - precisionscore: 0.9821 - recallscore: 0.9779 - val_loss: 0.0514 - val_accuracy: 0.9851 - val_f1score: 0.9850 - val_precisionscore: 0.9867 - val_recallscore: 0.9833\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0691 - accuracy: 0.9789 - f1score: 0.9790 - precisionscore: 0.9811 - recallscore: 0.9770 - val_loss: 0.0477 - val_accuracy: 0.9868 - val_f1score: 0.9871 - val_precisionscore: 0.9888 - val_recallscore: 0.9854\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0684 - accuracy: 0.9795 - f1score: 0.9796 - precisionscore: 0.9814 - recallscore: 0.9779 - val_loss: 0.0488 - val_accuracy: 0.9869 - val_f1score: 0.9870 - val_precisionscore: 0.9891 - val_recallscore: 0.9849\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0657 - accuracy: 0.9803 - f1score: 0.9806 - precisionscore: 0.9822 - recallscore: 0.9790 - val_loss: 0.0504 - val_accuracy: 0.9857 - val_f1score: 0.9859 - val_precisionscore: 0.9882 - val_recallscore: 0.9837\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0633 - accuracy: 0.9810 - f1score: 0.9810 - precisionscore: 0.9827 - recallscore: 0.9793 - val_loss: 0.0506 - val_accuracy: 0.9856 - val_f1score: 0.9857 - val_precisionscore: 0.9885 - val_recallscore: 0.9831\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0651 - accuracy: 0.9801 - f1score: 0.9804 - precisionscore: 0.9821 - recallscore: 0.9787 - val_loss: 0.0474 - val_accuracy: 0.9866 - val_f1score: 0.9869 - val_precisionscore: 0.9892 - val_recallscore: 0.9846\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0678 - accuracy: 0.9799 - f1score: 0.9800 - precisionscore: 0.9818 - recallscore: 0.9782 - val_loss: 0.0494 - val_accuracy: 0.9859 - val_f1score: 0.9862 - val_precisionscore: 0.9877 - val_recallscore: 0.9847\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0633 - accuracy: 0.9815 - f1score: 0.9816 - precisionscore: 0.9832 - recallscore: 0.9800 - val_loss: 0.0497 - val_accuracy: 0.9854 - val_f1score: 0.9858 - val_precisionscore: 0.9876 - val_recallscore: 0.9841\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0648 - accuracy: 0.9803 - f1score: 0.9802 - precisionscore: 0.9821 - recallscore: 0.9784 - val_loss: 0.0503 - val_accuracy: 0.9861 - val_f1score: 0.9859 - val_precisionscore: 0.9882 - val_recallscore: 0.9838\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0672 - accuracy: 0.9802 - f1score: 0.9804 - precisionscore: 0.9822 - recallscore: 0.9786 - val_loss: 0.0528 - val_accuracy: 0.9856 - val_f1score: 0.9852 - val_precisionscore: 0.9883 - val_recallscore: 0.9823\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0619 - accuracy: 0.9815 - f1score: 0.9818 - precisionscore: 0.9835 - recallscore: 0.9802 - val_loss: 0.0504 - val_accuracy: 0.9868 - val_f1score: 0.9865 - val_precisionscore: 0.9891 - val_recallscore: 0.9841\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0650 - accuracy: 0.9806 - f1score: 0.9806 - precisionscore: 0.9824 - recallscore: 0.9789 - val_loss: 0.0500 - val_accuracy: 0.9867 - val_f1score: 0.9866 - val_precisionscore: 0.9883 - val_recallscore: 0.9850\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0624 - accuracy: 0.9818 - f1score: 0.9818 - precisionscore: 0.9837 - recallscore: 0.9800 - val_loss: 0.0488 - val_accuracy: 0.9870 - val_f1score: 0.9866 - val_precisionscore: 0.9892 - val_recallscore: 0.9840\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0628 - accuracy: 0.9810 - f1score: 0.9812 - precisionscore: 0.9831 - recallscore: 0.9793 - val_loss: 0.0473 - val_accuracy: 0.9874 - val_f1score: 0.9868 - val_precisionscore: 0.9883 - val_recallscore: 0.9854\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0622 - accuracy: 0.9817 - f1score: 0.9819 - precisionscore: 0.9836 - recallscore: 0.9802 - val_loss: 0.0461 - val_accuracy: 0.9882 - val_f1score: 0.9878 - val_precisionscore: 0.9902 - val_recallscore: 0.9855\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0642 - accuracy: 0.9813 - f1score: 0.9814 - precisionscore: 0.9832 - recallscore: 0.9797 - val_loss: 0.0483 - val_accuracy: 0.9863 - val_f1score: 0.9860 - val_precisionscore: 0.9882 - val_recallscore: 0.9838\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0611 - accuracy: 0.9820 - f1score: 0.9820 - precisionscore: 0.9835 - recallscore: 0.9806 - val_loss: 0.0538 - val_accuracy: 0.9846 - val_f1score: 0.9845 - val_precisionscore: 0.9867 - val_recallscore: 0.9823\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0643 - accuracy: 0.9817 - f1score: 0.9817 - precisionscore: 0.9835 - recallscore: 0.9800 - val_loss: 0.0489 - val_accuracy: 0.9869 - val_f1score: 0.9866 - val_precisionscore: 0.9891 - val_recallscore: 0.9841\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0625 - accuracy: 0.9814 - f1score: 0.9817 - precisionscore: 0.9834 - recallscore: 0.9800 - val_loss: 0.0500 - val_accuracy: 0.9863 - val_f1score: 0.9868 - val_precisionscore: 0.9899 - val_recallscore: 0.9838\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0652 - accuracy: 0.9816 - f1score: 0.9815 - precisionscore: 0.9834 - recallscore: 0.9798 - val_loss: 0.0493 - val_accuracy: 0.9880 - val_f1score: 0.9872 - val_precisionscore: 0.9900 - val_recallscore: 0.9846\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0651 - accuracy: 0.9811 - f1score: 0.9812 - precisionscore: 0.9829 - recallscore: 0.9795 - val_loss: 0.0485 - val_accuracy: 0.9874 - val_f1score: 0.9869 - val_precisionscore: 0.9891 - val_recallscore: 0.9847\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0649 - accuracy: 0.9812 - f1score: 0.9812 - precisionscore: 0.9829 - recallscore: 0.9796 - val_loss: 0.0500 - val_accuracy: 0.9865 - val_f1score: 0.9865 - val_precisionscore: 0.9891 - val_recallscore: 0.9839\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0632 - accuracy: 0.9819 - f1score: 0.9819 - precisionscore: 0.9833 - recallscore: 0.9805 - val_loss: 0.0530 - val_accuracy: 0.9851 - val_f1score: 0.9850 - val_precisionscore: 0.9872 - val_recallscore: 0.9829\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0650 - accuracy: 0.9806 - f1score: 0.9809 - precisionscore: 0.9828 - recallscore: 0.9791 - val_loss: 0.0504 - val_accuracy: 0.9855 - val_f1score: 0.9852 - val_precisionscore: 0.9885 - val_recallscore: 0.9820\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0604 - accuracy: 0.9826 - f1score: 0.9825 - precisionscore: 0.9841 - recallscore: 0.9810 - val_loss: 0.0529 - val_accuracy: 0.9861 - val_f1score: 0.9857 - val_precisionscore: 0.9894 - val_recallscore: 0.9821\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0610 - accuracy: 0.9822 - f1score: 0.9822 - precisionscore: 0.9838 - recallscore: 0.9808 - val_loss: 0.0511 - val_accuracy: 0.9851 - val_f1score: 0.9853 - val_precisionscore: 0.9875 - val_recallscore: 0.9832\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0656 - accuracy: 0.9807 - f1score: 0.9810 - precisionscore: 0.9828 - recallscore: 0.9792 - val_loss: 0.0519 - val_accuracy: 0.9861 - val_f1score: 0.9861 - val_precisionscore: 0.9893 - val_recallscore: 0.9829\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0602 - accuracy: 0.9824 - f1score: 0.9824 - precisionscore: 0.9839 - recallscore: 0.9809 - val_loss: 0.0540 - val_accuracy: 0.9845 - val_f1score: 0.9848 - val_precisionscore: 0.9877 - val_recallscore: 0.9819\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0631 - accuracy: 0.9821 - f1score: 0.9823 - precisionscore: 0.9839 - recallscore: 0.9807 - val_loss: 0.0509 - val_accuracy: 0.9863 - val_f1score: 0.9859 - val_precisionscore: 0.9889 - val_recallscore: 0.9830\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0665 - accuracy: 0.9810 - f1score: 0.9809 - precisionscore: 0.9826 - recallscore: 0.9793 - val_loss: 0.0521 - val_accuracy: 0.9852 - val_f1score: 0.9852 - val_precisionscore: 0.9886 - val_recallscore: 0.9819\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0642 - accuracy: 0.9820 - f1score: 0.9822 - precisionscore: 0.9837 - recallscore: 0.9807 - val_loss: 0.0550 - val_accuracy: 0.9851 - val_f1score: 0.9849 - val_precisionscore: 0.9876 - val_recallscore: 0.9823\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0650 - accuracy: 0.9814 - f1score: 0.9813 - precisionscore: 0.9829 - recallscore: 0.9797 - val_loss: 0.0526 - val_accuracy: 0.9856 - val_f1score: 0.9853 - val_precisionscore: 0.9882 - val_recallscore: 0.9824\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0651 - accuracy: 0.9814 - f1score: 0.9814 - precisionscore: 0.9830 - recallscore: 0.9799 - val_loss: 0.0537 - val_accuracy: 0.9855 - val_f1score: 0.9848 - val_precisionscore: 0.9886 - val_recallscore: 0.9810\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0657 - accuracy: 0.9815 - f1score: 0.9813 - precisionscore: 0.9832 - recallscore: 0.9795 - val_loss: 0.0567 - val_accuracy: 0.9837 - val_f1score: 0.9844 - val_precisionscore: 0.9877 - val_recallscore: 0.9812\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0647 - accuracy: 0.9816 - f1score: 0.9817 - precisionscore: 0.9835 - recallscore: 0.9800 - val_loss: 0.0507 - val_accuracy: 0.9858 - val_f1score: 0.9860 - val_precisionscore: 0.9882 - val_recallscore: 0.9839\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0639 - accuracy: 0.9816 - f1score: 0.9816 - precisionscore: 0.9834 - recallscore: 0.9799 - val_loss: 0.0577 - val_accuracy: 0.9846 - val_f1score: 0.9838 - val_precisionscore: 0.9880 - val_recallscore: 0.9797\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0653 - accuracy: 0.9807 - f1score: 0.9807 - precisionscore: 0.9824 - recallscore: 0.9791 - val_loss: 0.0539 - val_accuracy: 0.9860 - val_f1score: 0.9852 - val_precisionscore: 0.9878 - val_recallscore: 0.9826\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0651 - accuracy: 0.9821 - f1score: 0.9822 - precisionscore: 0.9836 - recallscore: 0.9808 - val_loss: 0.0550 - val_accuracy: 0.9855 - val_f1score: 0.9850 - val_precisionscore: 0.9875 - val_recallscore: 0.9825\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0680 - accuracy: 0.9817 - f1score: 0.9819 - precisionscore: 0.9836 - recallscore: 0.9803 - val_loss: 0.0553 - val_accuracy: 0.9848 - val_f1score: 0.9839 - val_precisionscore: 0.9878 - val_recallscore: 0.9801\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0662 - accuracy: 0.9822 - f1score: 0.9819 - precisionscore: 0.9837 - recallscore: 0.9801 - val_loss: 0.0574 - val_accuracy: 0.9851 - val_f1score: 0.9843 - val_precisionscore: 0.9884 - val_recallscore: 0.9804\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0668 - accuracy: 0.9807 - f1score: 0.9807 - precisionscore: 0.9824 - recallscore: 0.9791 - val_loss: 0.0627 - val_accuracy: 0.9844 - val_f1score: 0.9835 - val_precisionscore: 0.9884 - val_recallscore: 0.9788\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0668 - accuracy: 0.9813 - f1score: 0.9813 - precisionscore: 0.9831 - recallscore: 0.9796 - val_loss: 0.0548 - val_accuracy: 0.9859 - val_f1score: 0.9853 - val_precisionscore: 0.9888 - val_recallscore: 0.9819\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0660 - accuracy: 0.9817 - f1score: 0.9816 - precisionscore: 0.9833 - recallscore: 0.9799 - val_loss: 0.0518 - val_accuracy: 0.9857 - val_f1score: 0.9852 - val_precisionscore: 0.9882 - val_recallscore: 0.9824\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0654 - accuracy: 0.9818 - f1score: 0.9818 - precisionscore: 0.9835 - recallscore: 0.9801 - val_loss: 0.0555 - val_accuracy: 0.9853 - val_f1score: 0.9839 - val_precisionscore: 0.9874 - val_recallscore: 0.9804\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0662 - accuracy: 0.9826 - f1score: 0.9825 - precisionscore: 0.9840 - recallscore: 0.9811 - val_loss: 0.0558 - val_accuracy: 0.9845 - val_f1score: 0.9841 - val_precisionscore: 0.9871 - val_recallscore: 0.9812\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0679 - accuracy: 0.9815 - f1score: 0.9814 - precisionscore: 0.9832 - recallscore: 0.9797 - val_loss: 0.0505 - val_accuracy: 0.9854 - val_f1score: 0.9856 - val_precisionscore: 0.9881 - val_recallscore: 0.9832\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0683 - accuracy: 0.9815 - f1score: 0.9814 - precisionscore: 0.9832 - recallscore: 0.9797 - val_loss: 0.0551 - val_accuracy: 0.9854 - val_f1score: 0.9856 - val_precisionscore: 0.9886 - val_recallscore: 0.9826\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0672 - accuracy: 0.9818 - f1score: 0.9819 - precisionscore: 0.9836 - recallscore: 0.9802 - val_loss: 0.0527 - val_accuracy: 0.9853 - val_f1score: 0.9849 - val_precisionscore: 0.9874 - val_recallscore: 0.9824\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0664 - accuracy: 0.9810 - f1score: 0.9812 - precisionscore: 0.9829 - recallscore: 0.9794 - val_loss: 0.0530 - val_accuracy: 0.9866 - val_f1score: 0.9864 - val_precisionscore: 0.9893 - val_recallscore: 0.9836\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0660 - accuracy: 0.9820 - f1score: 0.9822 - precisionscore: 0.9838 - recallscore: 0.9806 - val_loss: 0.0621 - val_accuracy: 0.9850 - val_f1score: 0.9836 - val_precisionscore: 0.9881 - val_recallscore: 0.9792\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0657 - accuracy: 0.9816 - f1score: 0.9817 - precisionscore: 0.9834 - recallscore: 0.9800 - val_loss: 0.0555 - val_accuracy: 0.9861 - val_f1score: 0.9854 - val_precisionscore: 0.9883 - val_recallscore: 0.9825\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0670 - accuracy: 0.9810 - f1score: 0.9810 - precisionscore: 0.9827 - recallscore: 0.9794 - val_loss: 0.0560 - val_accuracy: 0.9846 - val_f1score: 0.9841 - val_precisionscore: 0.9872 - val_recallscore: 0.9811\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0671 - accuracy: 0.9813 - f1score: 0.9815 - precisionscore: 0.9833 - recallscore: 0.9796 - val_loss: 0.0561 - val_accuracy: 0.9854 - val_f1score: 0.9846 - val_precisionscore: 0.9882 - val_recallscore: 0.9811\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0655 - accuracy: 0.9821 - f1score: 0.9820 - precisionscore: 0.9836 - recallscore: 0.9805 - val_loss: 0.0520 - val_accuracy: 0.9861 - val_f1score: 0.9859 - val_precisionscore: 0.9889 - val_recallscore: 0.9830\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0673 - accuracy: 0.9814 - f1score: 0.9815 - precisionscore: 0.9835 - recallscore: 0.9796 - val_loss: 0.0627 - val_accuracy: 0.9844 - val_f1score: 0.9831 - val_precisionscore: 0.9885 - val_recallscore: 0.9779\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0685 - accuracy: 0.9812 - f1score: 0.9815 - precisionscore: 0.9834 - recallscore: 0.9797 - val_loss: 0.0602 - val_accuracy: 0.9863 - val_f1score: 0.9847 - val_precisionscore: 0.9897 - val_recallscore: 0.9798\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0722 - accuracy: 0.9805 - f1score: 0.9806 - precisionscore: 0.9826 - recallscore: 0.9787 - val_loss: 0.0622 - val_accuracy: 0.9844 - val_f1score: 0.9830 - val_precisionscore: 0.9881 - val_recallscore: 0.9781\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0674 - accuracy: 0.9818 - f1score: 0.9819 - precisionscore: 0.9835 - recallscore: 0.9803 - val_loss: 0.0560 - val_accuracy: 0.9848 - val_f1score: 0.9848 - val_precisionscore: 0.9876 - val_recallscore: 0.9821\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0698 - accuracy: 0.9814 - f1score: 0.9813 - precisionscore: 0.9831 - recallscore: 0.9795 - val_loss: 0.0594 - val_accuracy: 0.9843 - val_f1score: 0.9835 - val_precisionscore: 0.9878 - val_recallscore: 0.9794\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0688 - accuracy: 0.9814 - f1score: 0.9815 - precisionscore: 0.9831 - recallscore: 0.9799 - val_loss: 0.0552 - val_accuracy: 0.9856 - val_f1score: 0.9851 - val_precisionscore: 0.9881 - val_recallscore: 0.9823\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0712 - accuracy: 0.9799 - f1score: 0.9801 - precisionscore: 0.9820 - recallscore: 0.9783 - val_loss: 0.0671 - val_accuracy: 0.9847 - val_f1score: 0.9834 - val_precisionscore: 0.9891 - val_recallscore: 0.9780\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.0669 - accuracy: 0.9813 - f1score: 0.9812 - precisionscore: 0.9828 - recallscore: 0.9796 - val_loss: 0.0558 - val_accuracy: 0.9838 - val_f1score: 0.9835 - val_precisionscore: 0.9863 - val_recallscore: 0.9808\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0720 - accuracy: 0.9806 - f1score: 0.9808 - precisionscore: 0.9828 - recallscore: 0.9789 - val_loss: 0.0593 - val_accuracy: 0.9848 - val_f1score: 0.9844 - val_precisionscore: 0.9885 - val_recallscore: 0.9804\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0708 - accuracy: 0.9806 - f1score: 0.9806 - precisionscore: 0.9823 - recallscore: 0.9789 - val_loss: 0.0524 - val_accuracy: 0.9858 - val_f1score: 0.9857 - val_precisionscore: 0.9889 - val_recallscore: 0.9825\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0699 - accuracy: 0.9814 - f1score: 0.9813 - precisionscore: 0.9833 - recallscore: 0.9794 - val_loss: 0.0561 - val_accuracy: 0.9855 - val_f1score: 0.9849 - val_precisionscore: 0.9888 - val_recallscore: 0.9812\n",
            "precompute_adjacency_images\n",
            "tensor([[ 0.3400, -0.0852, -0.1736, -0.1938,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.0852,  0.2413, -0.0987, -0.1763, -0.1944,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.1736, -0.0987,  0.2207, -0.1015, -0.1768, -0.1946,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.1938, -0.1763, -0.1015,  0.2166, -0.1020, -0.1770, -0.1946,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.0000, -0.1944, -0.1768, -0.1020,  0.2166, -0.1020, -0.1770, -0.1946,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020, -0.1770,\n",
            "         -0.1946,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020,\n",
            "         -0.1770, -0.1946],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166,\n",
            "         -0.1020, -0.1770],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,\n",
            "          0.2166, -0.1020],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770,\n",
            "         -0.1020,  0.2166]])\n",
            "number of trainable parameters: 7840\n",
            "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 0.937903, Accuracy: (47.62%) F1 Score: (47.31%) Precision: (47.26%) Recall: (47.36%)\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.723365, Accuracy: (60.88%) F1 Score: (60.57%) Precision: (60.65%) Recall: (60.50%)\n",
            "Train Epoch: 1 [29984/60000 (50%)]\tLoss: 0.770608, Accuracy: (67.09%) F1 Score: (66.84%) Precision: (66.94%) Recall: (66.74%)\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.724414, Accuracy: (70.82%) F1 Score: (70.59%) Precision: (70.72%) Recall: (70.47%)\n",
            "Train Epoch: 1 [49984/60000 (83%)]\tLoss: 0.606221, Accuracy: (73.35%) F1 Score: (73.11%) Precision: (73.24%) Recall: (72.98%)\n",
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.517183, Accuracy: (75.15%) F1 Score: (73.11%) Precision: (73.24%) Recall: (72.98%)\n",
            "\n",
            "Test set: Average loss: 0.5283, Accuracy: 8504/10000 (85.04%)\n",
            "\n",
            "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.538366, Accuracy: (84.86%) F1 Score: (84.76%) Precision: (84.83%) Recall: (84.69%)\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.566497, Accuracy: (84.89%) F1 Score: (84.73%) Precision: (84.85%) Recall: (84.62%)\n",
            "Train Epoch: 2 [29984/60000 (50%)]\tLoss: 0.264310, Accuracy: (85.07%) F1 Score: (84.90%) Precision: (85.00%) Recall: (84.79%)\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.312043, Accuracy: (85.44%) F1 Score: (85.29%) Precision: (85.38%) Recall: (85.20%)\n",
            "Train Epoch: 2 [49984/60000 (83%)]\tLoss: 0.607663, Accuracy: (85.49%) F1 Score: (85.33%) Precision: (85.41%) Recall: (85.25%)\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.368691, Accuracy: (85.74%) F1 Score: (85.33%) Precision: (85.41%) Recall: (85.25%)\n",
            "\n",
            "Test set: Average loss: 0.4472, Accuracy: 8709/10000 (87.09%)\n",
            "\n",
            "Train Epoch: 3 [9984/60000 (17%)]\tLoss: 0.300301, Accuracy: (86.63%) F1 Score: (86.42%) Precision: (86.48%) Recall: (86.37%)\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.275162, Accuracy: (87.01%) F1 Score: (86.92%) Precision: (86.96%) Recall: (86.88%)\n",
            "Train Epoch: 3 [29984/60000 (50%)]\tLoss: 0.466556, Accuracy: (87.01%) F1 Score: (86.92%) Precision: (86.96%) Recall: (86.88%)\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.512877, Accuracy: (87.05%) F1 Score: (86.94%) Precision: (86.98%) Recall: (86.90%)\n",
            "Train Epoch: 3 [49984/60000 (83%)]\tLoss: 0.248219, Accuracy: (87.02%) F1 Score: (86.89%) Precision: (86.94%) Recall: (86.84%)\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.452020, Accuracy: (87.09%) F1 Score: (86.89%) Precision: (86.94%) Recall: (86.84%)\n",
            "\n",
            "Test set: Average loss: 0.4148, Accuracy: 8775/10000 (87.75%)\n",
            "\n",
            "Train Epoch: 4 [9984/60000 (17%)]\tLoss: 0.426802, Accuracy: (87.10%) F1 Score: (86.98%) Precision: (86.98%) Recall: (86.98%)\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.437034, Accuracy: (87.50%) F1 Score: (87.38%) Precision: (87.40%) Recall: (87.36%)\n",
            "Train Epoch: 4 [29984/60000 (50%)]\tLoss: 0.288291, Accuracy: (87.65%) F1 Score: (87.51%) Precision: (87.55%) Recall: (87.47%)\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.341598, Accuracy: (87.68%) F1 Score: (87.55%) Precision: (87.58%) Recall: (87.51%)\n",
            "Train Epoch: 4 [49984/60000 (83%)]\tLoss: 0.585428, Accuracy: (87.72%) F1 Score: (87.58%) Precision: (87.62%) Recall: (87.54%)\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.538720, Accuracy: (87.78%) F1 Score: (87.58%) Precision: (87.62%) Recall: (87.54%)\n",
            "\n",
            "Test set: Average loss: 0.3956, Accuracy: 8825/10000 (88.25%)\n",
            "\n",
            "Train Epoch: 5 [9984/60000 (17%)]\tLoss: 0.402891, Accuracy: (87.76%) F1 Score: (87.62%) Precision: (87.69%) Recall: (87.55%)\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.677179, Accuracy: (88.09%) F1 Score: (87.91%) Precision: (87.96%) Recall: (87.87%)\n",
            "Train Epoch: 5 [29984/60000 (50%)]\tLoss: 0.389663, Accuracy: (88.33%) F1 Score: (88.16%) Precision: (88.21%) Recall: (88.11%)\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.357403, Accuracy: (88.27%) F1 Score: (88.12%) Precision: (88.16%) Recall: (88.08%)\n",
            "Train Epoch: 5 [49984/60000 (83%)]\tLoss: 0.599613, Accuracy: (88.19%) F1 Score: (88.05%) Precision: (88.08%) Recall: (88.01%)\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.413530, Accuracy: (88.20%) F1 Score: (88.05%) Precision: (88.08%) Recall: (88.01%)\n",
            "\n",
            "Test set: Average loss: 0.3801, Accuracy: 8875/10000 (88.75%)\n",
            "\n",
            "Train Epoch: 6 [9984/60000 (17%)]\tLoss: 0.388584, Accuracy: (88.17%) F1 Score: (87.95%) Precision: (88.01%) Recall: (87.90%)\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.545533, Accuracy: (88.37%) F1 Score: (88.22%) Precision: (88.26%) Recall: (88.19%)\n",
            "Train Epoch: 6 [29984/60000 (50%)]\tLoss: 0.338105, Accuracy: (88.52%) F1 Score: (88.40%) Precision: (88.43%) Recall: (88.37%)\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.388639, Accuracy: (88.50%) F1 Score: (88.36%) Precision: (88.39%) Recall: (88.33%)\n",
            "Train Epoch: 6 [49984/60000 (83%)]\tLoss: 0.675796, Accuracy: (88.58%) F1 Score: (88.44%) Precision: (88.48%) Recall: (88.41%)\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.707853, Accuracy: (88.51%) F1 Score: (88.44%) Precision: (88.48%) Recall: (88.41%)\n",
            "\n",
            "Test set: Average loss: 0.3706, Accuracy: 8904/10000 (89.04%)\n",
            "\n",
            "Train Epoch: 7 [9984/60000 (17%)]\tLoss: 0.206045, Accuracy: (88.90%) F1 Score: (88.76%) Precision: (88.79%) Recall: (88.73%)\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.286526, Accuracy: (88.91%) F1 Score: (88.76%) Precision: (88.79%) Recall: (88.73%)\n",
            "Train Epoch: 7 [29984/60000 (50%)]\tLoss: 0.211943, Accuracy: (88.86%) F1 Score: (88.71%) Precision: (88.73%) Recall: (88.69%)\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.426800, Accuracy: (88.90%) F1 Score: (88.75%) Precision: (88.77%) Recall: (88.73%)\n",
            "Train Epoch: 7 [49984/60000 (83%)]\tLoss: 0.810043, Accuracy: (88.95%) F1 Score: (88.81%) Precision: (88.82%) Recall: (88.79%)\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.346573, Accuracy: (88.88%) F1 Score: (88.81%) Precision: (88.82%) Recall: (88.79%)\n",
            "\n",
            "Test set: Average loss: 0.3635, Accuracy: 8936/10000 (89.36%)\n",
            "\n",
            "Train Epoch: 8 [9984/60000 (17%)]\tLoss: 0.205476, Accuracy: (88.98%) F1 Score: (88.78%) Precision: (88.80%) Recall: (88.75%)\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.341877, Accuracy: (88.70%) F1 Score: (88.51%) Precision: (88.54%) Recall: (88.49%)\n",
            "Train Epoch: 8 [29984/60000 (50%)]\tLoss: 0.429430, Accuracy: (88.78%) F1 Score: (88.62%) Precision: (88.65%) Recall: (88.60%)\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.300577, Accuracy: (88.83%) F1 Score: (88.69%) Precision: (88.71%) Recall: (88.67%)\n",
            "Train Epoch: 8 [49984/60000 (83%)]\tLoss: 0.393463, Accuracy: (89.00%) F1 Score: (88.86%) Precision: (88.89%) Recall: (88.84%)\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.799199, Accuracy: (89.01%) F1 Score: (88.86%) Precision: (88.89%) Recall: (88.84%)\n",
            "\n",
            "Test set: Average loss: 0.3565, Accuracy: 8970/10000 (89.70%)\n",
            "\n",
            "Train Epoch: 9 [9984/60000 (17%)]\tLoss: 0.323651, Accuracy: (89.43%) F1 Score: (89.30%) Precision: (89.33%) Recall: (89.28%)\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.270700, Accuracy: (89.04%) F1 Score: (88.88%) Precision: (88.90%) Recall: (88.86%)\n",
            "Train Epoch: 9 [29984/60000 (50%)]\tLoss: 0.352243, Accuracy: (89.12%) F1 Score: (88.95%) Precision: (88.98%) Recall: (88.93%)\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.269740, Accuracy: (89.11%) F1 Score: (88.96%) Precision: (88.98%) Recall: (88.94%)\n",
            "Train Epoch: 9 [49984/60000 (83%)]\tLoss: 0.377217, Accuracy: (89.16%) F1 Score: (89.03%) Precision: (89.05%) Recall: (89.00%)\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.262728, Accuracy: (89.24%) F1 Score: (89.03%) Precision: (89.05%) Recall: (89.00%)\n",
            "\n",
            "Test set: Average loss: 0.3510, Accuracy: 8976/10000 (89.76%)\n",
            "\n",
            "Train Epoch: 10 [9984/60000 (17%)]\tLoss: 0.378270, Accuracy: (89.50%) F1 Score: (89.34%) Precision: (89.39%) Recall: (89.30%)\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.590073, Accuracy: (89.13%) F1 Score: (88.97%) Precision: (89.01%) Recall: (88.93%)\n",
            "Train Epoch: 10 [29984/60000 (50%)]\tLoss: 0.345359, Accuracy: (89.11%) F1 Score: (88.96%) Precision: (88.99%) Recall: (88.93%)\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.180153, Accuracy: (89.29%) F1 Score: (89.14%) Precision: (89.17%) Recall: (89.11%)\n",
            "Train Epoch: 10 [49984/60000 (83%)]\tLoss: 0.397925, Accuracy: (89.26%) F1 Score: (89.11%) Precision: (89.14%) Recall: (89.09%)\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.340702, Accuracy: (89.29%) F1 Score: (89.11%) Precision: (89.14%) Recall: (89.09%)\n",
            "\n",
            "Test set: Average loss: 0.3474, Accuracy: 9002/10000 (90.02%)\n",
            "\n",
            "Train Epoch: 11 [9984/60000 (17%)]\tLoss: 0.323031, Accuracy: (89.79%) F1 Score: (89.61%) Precision: (89.62%) Recall: (89.61%)\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.314165, Accuracy: (89.62%) F1 Score: (89.44%) Precision: (89.46%) Recall: (89.43%)\n",
            "Train Epoch: 11 [29984/60000 (50%)]\tLoss: 0.528741, Accuracy: (89.61%) F1 Score: (89.44%) Precision: (89.46%) Recall: (89.43%)\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.239860, Accuracy: (89.44%) F1 Score: (89.30%) Precision: (89.31%) Recall: (89.28%)\n",
            "Train Epoch: 11 [49984/60000 (83%)]\tLoss: 0.387762, Accuracy: (89.45%) F1 Score: (89.32%) Precision: (89.33%) Recall: (89.31%)\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.098481, Accuracy: (89.51%) F1 Score: (89.32%) Precision: (89.33%) Recall: (89.31%)\n",
            "\n",
            "Test set: Average loss: 0.3427, Accuracy: 9004/10000 (90.04%)\n",
            "\n",
            "Train Epoch: 12 [9984/60000 (17%)]\tLoss: 0.297458, Accuracy: (89.25%) F1 Score: (89.10%) Precision: (89.13%) Recall: (89.08%)\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.309666, Accuracy: (89.63%) F1 Score: (89.47%) Precision: (89.50%) Recall: (89.45%)\n",
            "Train Epoch: 12 [29984/60000 (50%)]\tLoss: 0.305365, Accuracy: (89.57%) F1 Score: (89.43%) Precision: (89.46%) Recall: (89.41%)\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.338821, Accuracy: (89.52%) F1 Score: (89.39%) Precision: (89.41%) Recall: (89.36%)\n",
            "Train Epoch: 12 [49984/60000 (83%)]\tLoss: 0.469859, Accuracy: (89.62%) F1 Score: (89.50%) Precision: (89.52%) Recall: (89.48%)\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.140930, Accuracy: (89.59%) F1 Score: (89.50%) Precision: (89.52%) Recall: (89.48%)\n",
            "\n",
            "Test set: Average loss: 0.3407, Accuracy: 9011/10000 (90.11%)\n",
            "\n",
            "Train Epoch: 13 [9984/60000 (17%)]\tLoss: 0.234551, Accuracy: (89.56%) F1 Score: (89.44%) Precision: (89.46%) Recall: (89.42%)\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.356495, Accuracy: (89.71%) F1 Score: (89.53%) Precision: (89.56%) Recall: (89.51%)\n",
            "Train Epoch: 13 [29984/60000 (50%)]\tLoss: 0.484703, Accuracy: (89.65%) F1 Score: (89.51%) Precision: (89.53%) Recall: (89.50%)\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.399316, Accuracy: (89.68%) F1 Score: (89.55%) Precision: (89.57%) Recall: (89.53%)\n",
            "Train Epoch: 13 [49984/60000 (83%)]\tLoss: 0.368109, Accuracy: (89.61%) F1 Score: (89.48%) Precision: (89.50%) Recall: (89.47%)\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.160676, Accuracy: (89.63%) F1 Score: (89.48%) Precision: (89.50%) Recall: (89.47%)\n",
            "\n",
            "Test set: Average loss: 0.3370, Accuracy: 9018/10000 (90.18%)\n",
            "\n",
            "Train Epoch: 14 [9984/60000 (17%)]\tLoss: 0.298698, Accuracy: (89.65%) F1 Score: (89.54%) Precision: (89.55%) Recall: (89.53%)\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.233676, Accuracy: (89.69%) F1 Score: (89.58%) Precision: (89.58%) Recall: (89.58%)\n",
            "Train Epoch: 14 [29984/60000 (50%)]\tLoss: 0.234538, Accuracy: (89.67%) F1 Score: (89.56%) Precision: (89.56%) Recall: (89.56%)\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.453393, Accuracy: (89.71%) F1 Score: (89.59%) Precision: (89.60%) Recall: (89.58%)\n",
            "Train Epoch: 14 [49984/60000 (83%)]\tLoss: 0.596743, Accuracy: (89.72%) F1 Score: (89.60%) Precision: (89.62%) Recall: (89.59%)\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.422681, Accuracy: (89.75%) F1 Score: (89.60%) Precision: (89.62%) Recall: (89.59%)\n",
            "\n",
            "Test set: Average loss: 0.3363, Accuracy: 9014/10000 (90.14%)\n",
            "\n",
            "Train Epoch: 15 [9984/60000 (17%)]\tLoss: 0.454769, Accuracy: (89.85%) F1 Score: (89.73%) Precision: (89.76%) Recall: (89.70%)\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.304995, Accuracy: (90.17%) F1 Score: (90.02%) Precision: (90.05%) Recall: (89.98%)\n",
            "Train Epoch: 15 [29984/60000 (50%)]\tLoss: 0.238883, Accuracy: (90.02%) F1 Score: (89.87%) Precision: (89.90%) Recall: (89.85%)\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.526320, Accuracy: (89.91%) F1 Score: (89.77%) Precision: (89.79%) Recall: (89.75%)\n",
            "Train Epoch: 15 [49984/60000 (83%)]\tLoss: 0.331693, Accuracy: (89.83%) F1 Score: (89.69%) Precision: (89.71%) Recall: (89.67%)\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.264723, Accuracy: (89.81%) F1 Score: (89.69%) Precision: (89.71%) Recall: (89.67%)\n",
            "\n",
            "Test set: Average loss: 0.3306, Accuracy: 9040/10000 (90.40%)\n",
            "\n",
            "Train Epoch: 16 [9984/60000 (17%)]\tLoss: 0.253344, Accuracy: (89.78%) F1 Score: (89.63%) Precision: (89.65%) Recall: (89.61%)\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.296310, Accuracy: (89.77%) F1 Score: (89.66%) Precision: (89.68%) Recall: (89.63%)\n",
            "Train Epoch: 16 [29984/60000 (50%)]\tLoss: 0.291874, Accuracy: (90.01%) F1 Score: (89.89%) Precision: (89.91%) Recall: (89.87%)\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.292633, Accuracy: (89.91%) F1 Score: (89.78%) Precision: (89.80%) Recall: (89.76%)\n",
            "Train Epoch: 16 [49984/60000 (83%)]\tLoss: 0.274021, Accuracy: (89.83%) F1 Score: (89.70%) Precision: (89.72%) Recall: (89.69%)\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.344240, Accuracy: (89.82%) F1 Score: (89.70%) Precision: (89.72%) Recall: (89.69%)\n",
            "\n",
            "Test set: Average loss: 0.3297, Accuracy: 9042/10000 (90.42%)\n",
            "\n",
            "Train Epoch: 17 [9984/60000 (17%)]\tLoss: 0.505753, Accuracy: (89.53%) F1 Score: (89.37%) Precision: (89.39%) Recall: (89.36%)\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.297709, Accuracy: (89.72%) F1 Score: (89.58%) Precision: (89.60%) Recall: (89.57%)\n",
            "Train Epoch: 17 [29984/60000 (50%)]\tLoss: 0.285511, Accuracy: (89.86%) F1 Score: (89.74%) Precision: (89.76%) Recall: (89.73%)\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.548244, Accuracy: (89.93%) F1 Score: (89.81%) Precision: (89.82%) Recall: (89.80%)\n",
            "Train Epoch: 17 [49984/60000 (83%)]\tLoss: 0.736933, Accuracy: (89.92%) F1 Score: (89.80%) Precision: (89.81%) Recall: (89.78%)\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.207248, Accuracy: (89.92%) F1 Score: (89.80%) Precision: (89.81%) Recall: (89.78%)\n",
            "\n",
            "Test set: Average loss: 0.3278, Accuracy: 9050/10000 (90.50%)\n",
            "\n",
            "Train Epoch: 18 [9984/60000 (17%)]\tLoss: 0.361665, Accuracy: (89.70%) F1 Score: (89.55%) Precision: (89.57%) Recall: (89.53%)\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.222653, Accuracy: (89.90%) F1 Score: (89.76%) Precision: (89.78%) Recall: (89.75%)\n",
            "Train Epoch: 18 [29984/60000 (50%)]\tLoss: 0.432201, Accuracy: (89.87%) F1 Score: (89.74%) Precision: (89.75%) Recall: (89.72%)\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.287973, Accuracy: (89.85%) F1 Score: (89.72%) Precision: (89.73%) Recall: (89.70%)\n",
            "Train Epoch: 18 [49984/60000 (83%)]\tLoss: 0.147926, Accuracy: (89.85%) F1 Score: (89.72%) Precision: (89.73%) Recall: (89.71%)\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.182688, Accuracy: (89.93%) F1 Score: (89.72%) Precision: (89.73%) Recall: (89.71%)\n",
            "\n",
            "Test set: Average loss: 0.3250, Accuracy: 9061/10000 (90.61%)\n",
            "\n",
            "Train Epoch: 19 [9984/60000 (17%)]\tLoss: 0.317505, Accuracy: (90.50%) F1 Score: (90.44%) Precision: (90.45%) Recall: (90.43%)\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.499292, Accuracy: (90.07%) F1 Score: (89.94%) Precision: (89.95%) Recall: (89.93%)\n",
            "Train Epoch: 19 [29984/60000 (50%)]\tLoss: 0.393269, Accuracy: (89.97%) F1 Score: (89.86%) Precision: (89.87%) Recall: (89.86%)\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.163112, Accuracy: (89.93%) F1 Score: (89.82%) Precision: (89.83%) Recall: (89.81%)\n",
            "Train Epoch: 19 [49984/60000 (83%)]\tLoss: 0.561373, Accuracy: (90.07%) F1 Score: (89.94%) Precision: (89.95%) Recall: (89.93%)\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.318111, Accuracy: (90.05%) F1 Score: (89.94%) Precision: (89.95%) Recall: (89.93%)\n",
            "\n",
            "Test set: Average loss: 0.3242, Accuracy: 9061/10000 (90.61%)\n",
            "\n",
            "Train Epoch: 20 [9984/60000 (17%)]\tLoss: 0.257749, Accuracy: (89.98%) F1 Score: (89.83%) Precision: (89.84%) Recall: (89.81%)\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.108400, Accuracy: (90.11%) F1 Score: (89.97%) Precision: (89.98%) Recall: (89.97%)\n",
            "Train Epoch: 20 [29984/60000 (50%)]\tLoss: 0.375476, Accuracy: (90.13%) F1 Score: (89.99%) Precision: (90.00%) Recall: (89.98%)\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.236557, Accuracy: (90.18%) F1 Score: (90.04%) Precision: (90.06%) Recall: (90.02%)\n",
            "Train Epoch: 20 [49984/60000 (83%)]\tLoss: 0.494095, Accuracy: (90.17%) F1 Score: (90.06%) Precision: (90.07%) Recall: (90.04%)\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.445670, Accuracy: (90.09%) F1 Score: (90.06%) Precision: (90.07%) Recall: (90.04%)\n",
            "\n",
            "Test set: Average loss: 0.3233, Accuracy: 9062/10000 (90.62%)\n",
            "\n",
            "Train Epoch: 21 [9984/60000 (17%)]\tLoss: 0.885033, Accuracy: (89.96%) F1 Score: (89.80%) Precision: (89.83%) Recall: (89.78%)\n",
            "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 0.361181, Accuracy: (90.20%) F1 Score: (90.02%) Precision: (90.04%) Recall: (90.00%)\n",
            "Train Epoch: 21 [29984/60000 (50%)]\tLoss: 0.518986, Accuracy: (90.06%) F1 Score: (89.88%) Precision: (89.90%) Recall: (89.86%)\n",
            "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.232367, Accuracy: (90.10%) F1 Score: (89.95%) Precision: (89.97%) Recall: (89.94%)\n",
            "Train Epoch: 21 [49984/60000 (83%)]\tLoss: 0.482212, Accuracy: (90.10%) F1 Score: (89.96%) Precision: (89.98%) Recall: (89.95%)\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.145716, Accuracy: (90.08%) F1 Score: (89.96%) Precision: (89.98%) Recall: (89.95%)\n",
            "\n",
            "Test set: Average loss: 0.3206, Accuracy: 9069/10000 (90.69%)\n",
            "\n",
            "Train Epoch: 22 [9984/60000 (17%)]\tLoss: 0.467413, Accuracy: (89.85%) F1 Score: (89.75%) Precision: (89.75%) Recall: (89.75%)\n",
            "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 0.316485, Accuracy: (89.96%) F1 Score: (89.86%) Precision: (89.87%) Recall: (89.85%)\n",
            "Train Epoch: 22 [29984/60000 (50%)]\tLoss: 0.628366, Accuracy: (90.16%) F1 Score: (90.05%) Precision: (90.06%) Recall: (90.04%)\n",
            "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 0.292234, Accuracy: (90.12%) F1 Score: (90.00%) Precision: (90.01%) Recall: (89.99%)\n",
            "Train Epoch: 22 [49984/60000 (83%)]\tLoss: 0.243703, Accuracy: (90.13%) F1 Score: (90.00%) Precision: (90.01%) Recall: (89.99%)\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.416828, Accuracy: (90.18%) F1 Score: (90.00%) Precision: (90.01%) Recall: (89.99%)\n",
            "\n",
            "Test set: Average loss: 0.3192, Accuracy: 9065/10000 (90.65%)\n",
            "\n",
            "Train Epoch: 23 [9984/60000 (17%)]\tLoss: 0.221643, Accuracy: (90.19%) F1 Score: (90.07%) Precision: (90.08%) Recall: (90.05%)\n",
            "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 0.357838, Accuracy: (90.09%) F1 Score: (89.97%) Precision: (89.98%) Recall: (89.96%)\n",
            "Train Epoch: 23 [29984/60000 (50%)]\tLoss: 0.074779, Accuracy: (90.20%) F1 Score: (90.08%) Precision: (90.09%) Recall: (90.06%)\n",
            "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.476346, Accuracy: (90.21%) F1 Score: (90.09%) Precision: (90.10%) Recall: (90.08%)\n",
            "Train Epoch: 23 [49984/60000 (83%)]\tLoss: 0.121761, Accuracy: (90.15%) F1 Score: (90.03%) Precision: (90.04%) Recall: (90.02%)\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.366705, Accuracy: (90.21%) F1 Score: (90.03%) Precision: (90.04%) Recall: (90.02%)\n",
            "\n",
            "Test set: Average loss: 0.3187, Accuracy: 9064/10000 (90.64%)\n",
            "\n",
            "Train Epoch: 24 [9984/60000 (17%)]\tLoss: 0.375131, Accuracy: (90.73%) F1 Score: (90.55%) Precision: (90.57%) Recall: (90.53%)\n",
            "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 0.437832, Accuracy: (90.29%) F1 Score: (90.14%) Precision: (90.15%) Recall: (90.13%)\n",
            "Train Epoch: 24 [29984/60000 (50%)]\tLoss: 0.570839, Accuracy: (90.27%) F1 Score: (90.14%) Precision: (90.15%) Recall: (90.13%)\n",
            "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.508083, Accuracy: (90.30%) F1 Score: (90.17%) Precision: (90.18%) Recall: (90.16%)\n",
            "Train Epoch: 24 [49984/60000 (83%)]\tLoss: 0.317000, Accuracy: (90.24%) F1 Score: (90.13%) Precision: (90.14%) Recall: (90.11%)\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.281508, Accuracy: (90.25%) F1 Score: (90.13%) Precision: (90.14%) Recall: (90.11%)\n",
            "\n",
            "Test set: Average loss: 0.3167, Accuracy: 9077/10000 (90.77%)\n",
            "\n",
            "Train Epoch: 25 [9984/60000 (17%)]\tLoss: 0.226538, Accuracy: (90.51%) F1 Score: (90.30%) Precision: (90.33%) Recall: (90.28%)\n",
            "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 0.413384, Accuracy: (90.45%) F1 Score: (90.31%) Precision: (90.33%) Recall: (90.29%)\n",
            "Train Epoch: 25 [29984/60000 (50%)]\tLoss: 0.253260, Accuracy: (90.29%) F1 Score: (90.15%) Precision: (90.16%) Recall: (90.14%)\n",
            "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.361372, Accuracy: (90.45%) F1 Score: (90.33%) Precision: (90.34%) Recall: (90.32%)\n",
            "Train Epoch: 25 [49984/60000 (83%)]\tLoss: 0.261251, Accuracy: (90.39%) F1 Score: (90.27%) Precision: (90.28%) Recall: (90.26%)\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.403477, Accuracy: (90.34%) F1 Score: (90.27%) Precision: (90.28%) Recall: (90.26%)\n",
            "\n",
            "Test set: Average loss: 0.3162, Accuracy: 9082/10000 (90.82%)\n",
            "\n",
            "Train Epoch: 26 [9984/60000 (17%)]\tLoss: 0.159044, Accuracy: (90.04%) F1 Score: (89.87%) Precision: (89.88%) Recall: (89.85%)\n",
            "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 0.219851, Accuracy: (90.24%) F1 Score: (90.14%) Precision: (90.15%) Recall: (90.14%)\n",
            "Train Epoch: 26 [29984/60000 (50%)]\tLoss: 0.729224, Accuracy: (90.31%) F1 Score: (90.19%) Precision: (90.20%) Recall: (90.18%)\n",
            "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 0.671392, Accuracy: (90.26%) F1 Score: (90.13%) Precision: (90.15%) Recall: (90.12%)\n",
            "Train Epoch: 26 [49984/60000 (83%)]\tLoss: 0.288436, Accuracy: (90.33%) F1 Score: (90.20%) Precision: (90.22%) Recall: (90.19%)\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.324934, Accuracy: (90.35%) F1 Score: (90.20%) Precision: (90.22%) Recall: (90.19%)\n",
            "\n",
            "Test set: Average loss: 0.3149, Accuracy: 9077/10000 (90.77%)\n",
            "\n",
            "Train Epoch: 27 [9984/60000 (17%)]\tLoss: 0.337121, Accuracy: (90.59%) F1 Score: (90.45%) Precision: (90.47%) Recall: (90.43%)\n",
            "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 0.263802, Accuracy: (90.18%) F1 Score: (90.04%) Precision: (90.06%) Recall: (90.03%)\n",
            "Train Epoch: 27 [29984/60000 (50%)]\tLoss: 0.525480, Accuracy: (90.18%) F1 Score: (90.06%) Precision: (90.07%) Recall: (90.04%)\n"
          ]
        }
      ]
    }
  ]
}